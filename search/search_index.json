{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Double Open aims to automate open source compliance for the Yocto Project . Overview Below is a high-level overview of the compliance workflow: sequenceDiagram autonumber Build Environment ->> Build Environment: Build the product with Yocto, create SPDX Document for the product Build Environment ->> Fossology: Upload source archives of the open-source components Fossology ->> Fossology: Scan the source for license and copyright data Build Environment ->> Fossology: Query for license and copyright data with file hash values Fossology ->> Build Environment: Return license and copyright data for the files Build Environment ->> Build Environment: Populate SPDX Document with the license and copyright data Build Environment ->> Build Environment: Convert SPDX Document to ORT's data format Build Environment ->> Build Environment: Evaluate compliance with ORT's Evaluator Build Environment ->> Build Environment: Create notice file and report with ORT's Reporter","title":"Home"},{"location":"#introduction","text":"Double Open aims to automate open source compliance for the Yocto Project .","title":"Introduction"},{"location":"#overview","text":"Below is a high-level overview of the compliance workflow: sequenceDiagram autonumber Build Environment ->> Build Environment: Build the product with Yocto, create SPDX Document for the product Build Environment ->> Fossology: Upload source archives of the open-source components Fossology ->> Fossology: Scan the source for license and copyright data Build Environment ->> Fossology: Query for license and copyright data with file hash values Fossology ->> Build Environment: Return license and copyright data for the files Build Environment ->> Build Environment: Populate SPDX Document with the license and copyright data Build Environment ->> Build Environment: Convert SPDX Document to ORT's data format Build Environment ->> Build Environment: Evaluate compliance with ORT's Evaluator Build Environment ->> Build Environment: Create notice file and report with ORT's Reporter","title":"Overview"},{"location":"ort/analyze/","text":"Analyze ORT Analyzer is used to determine the dependencies of projects in the analyzed repository. Analyzer also retrieves and stores metadata about the dependencies. The target of the ORT pipeline is a repository containing the source code of the projects to analyze. ORT supports and can detect dependencies managed by multiple package managers . Run analysis with the command: ort --info --force-overwrite analyze -i repo -o analyzer or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ analyze \\ -i /project/repo \\ -o /project/analyzer The Analyzer outputs the result in analyzer/analyzer-result.yml , which can then be used as an input for the Scanner. Labels Labels can be used to toggle evaluator rules based on product needs. The labels are toggled by using them as an input for Analyzer with for example --label nuclear=yes . Below are the labels supported by Double Open's Evaluator rules. Nuclear Some licenses forbid usage of the software in nuclear facilities. set label nuclear to yes to create violations for these licenses.","title":"Analyze"},{"location":"ort/analyze/#analyze","text":"ORT Analyzer is used to determine the dependencies of projects in the analyzed repository. Analyzer also retrieves and stores metadata about the dependencies. The target of the ORT pipeline is a repository containing the source code of the projects to analyze. ORT supports and can detect dependencies managed by multiple package managers . Run analysis with the command: ort --info --force-overwrite analyze -i repo -o analyzer or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ analyze \\ -i /project/repo \\ -o /project/analyzer The Analyzer outputs the result in analyzer/analyzer-result.yml , which can then be used as an input for the Scanner.","title":"Analyze"},{"location":"ort/analyze/#labels","text":"Labels can be used to toggle evaluator rules based on product needs. The labels are toggled by using them as an input for Analyzer with for example --label nuclear=yes . Below are the labels supported by Double Open's Evaluator rules.","title":"Labels"},{"location":"ort/analyze/#nuclear","text":"Some licenses forbid usage of the software in nuclear facilities. set label nuclear to yes to create violations for these licenses.","title":"Nuclear"},{"location":"ort/configuration/","text":"Configuration ORT uses text based configuration files to configure different aspects of the tool. We'll go over the different ways to configure the tooling below. Setup Clone ORT configuration repository to ~/.ort/config for ORT to automatically find the configuration files. Optional: configure scanner storage on Postgres or Artifactory by setting the details and credentials in .env file. Clone the target repository to $PWD/repo . The results of the pipeline will be placed next to that directory, eg. in $PWD/reporter . Configuration options Scanner storage ORT stores scanner results, so each run of the pipeline doesn't need to scan all dependencies again. The details and credentials for the storages are configured in the .env file. Without configuring either Postgres or Artifactory there, local file based storage is used, which is not persisted between Docker runs. License classification A license classification is needed for the ORT Evaluator to evaluate the project's compliance against a license policy. We publish our license classification on our Policy Configuration repo . If you want to use our license classification with your ORT configuration repository, we recommend cloning the Policy Configuration repo and linking the license-classifications.yml from the Policy Configuration to your ORT configuration directory when running ORT so ORT can automatically find the correct license classification.","title":"Configuration"},{"location":"ort/configuration/#configuration","text":"ORT uses text based configuration files to configure different aspects of the tool. We'll go over the different ways to configure the tooling below.","title":"Configuration"},{"location":"ort/configuration/#setup","text":"Clone ORT configuration repository to ~/.ort/config for ORT to automatically find the configuration files. Optional: configure scanner storage on Postgres or Artifactory by setting the details and credentials in .env file. Clone the target repository to $PWD/repo . The results of the pipeline will be placed next to that directory, eg. in $PWD/reporter .","title":"Setup"},{"location":"ort/configuration/#configuration-options","text":"","title":"Configuration options"},{"location":"ort/configuration/#scanner-storage","text":"ORT stores scanner results, so each run of the pipeline doesn't need to scan all dependencies again. The details and credentials for the storages are configured in the .env file. Without configuring either Postgres or Artifactory there, local file based storage is used, which is not persisted between Docker runs.","title":"Scanner storage"},{"location":"ort/configuration/#license-classification","text":"A license classification is needed for the ORT Evaluator to evaluate the project's compliance against a license policy. We publish our license classification on our Policy Configuration repo . If you want to use our license classification with your ORT configuration repository, we recommend cloning the Policy Configuration repo and linking the license-classifications.yml from the Policy Configuration to your ORT configuration directory when running ORT so ORT can automatically find the correct license classification.","title":"License classification"},{"location":"ort/evaluate/","text":"Evaluate Run evaluation with the command: ort --info --force-overwrite evaluate -i scanner/scan-result.yml -o evaluator or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ evaluate \\ -i /project/scanner/scan-result.yml \\ -o /project/evaluator The Evaluator outputs the result in evaluator/evaluation-result.yml , which can then be used as an input for the Reporter.","title":"Evaluate"},{"location":"ort/evaluate/#evaluate","text":"Run evaluation with the command: ort --info --force-overwrite evaluate -i scanner/scan-result.yml -o evaluator or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ evaluate \\ -i /project/scanner/scan-result.yml \\ -o /project/evaluator The Evaluator outputs the result in evaluator/evaluation-result.yml , which can then be used as an input for the Reporter.","title":"Evaluate"},{"location":"ort/installation/","text":"Installation Docker ORT is easiest to use with Docker. To use ORT with Docker, Docker needs to be installed on the host. We publish a pre-built Docker image of ORT on Docker Hub . To properly use ORT with Docker, some configuration files need to accessible inside the container. The most essential of these is the ORT configuration directory (defaults to ~/.ort/config ). Many package managers also have configuration files, which may set alternative addresses for package managers, or credentials for accessing them. A good starting point for passing a lot of the required configuration files is to mount the home directory of the user from the host machine to the container and setting the correct user in the container. An example for this can be found from ORT's example docker run script . Native Native installation may sometimes be simpler to run, for example if the setup of credentials and proxies is complicated. To use a native installation of ORT, the needed package managers need to be installed on the host. More information can be found from https://github.com/oss-review-toolkit/ort/#running-the-tools .","title":"Installation"},{"location":"ort/installation/#installation","text":"","title":"Installation"},{"location":"ort/installation/#docker","text":"ORT is easiest to use with Docker. To use ORT with Docker, Docker needs to be installed on the host. We publish a pre-built Docker image of ORT on Docker Hub . To properly use ORT with Docker, some configuration files need to accessible inside the container. The most essential of these is the ORT configuration directory (defaults to ~/.ort/config ). Many package managers also have configuration files, which may set alternative addresses for package managers, or credentials for accessing them. A good starting point for passing a lot of the required configuration files is to mount the home directory of the user from the host machine to the container and setting the correct user in the container. An example for this can be found from ORT's example docker run script .","title":"Docker"},{"location":"ort/installation/#native","text":"Native installation may sometimes be simpler to run, for example if the setup of credentials and proxies is complicated. To use a native installation of ORT, the needed package managers need to be installed on the host. More information can be found from https://github.com/oss-review-toolkit/ort/#running-the-tools .","title":"Native"},{"location":"ort/overview/","text":"Overview Double Open uses the OSS Review Toolkit (ORT) to analyze software projects and evaluate their open-source license compliance. The main ORT pipeline consist of four steps: Analyzer Scanner Evaluator Reporter After installing and configuring ORT, follow the above steps in order.","title":"Overview"},{"location":"ort/overview/#overview","text":"Double Open uses the OSS Review Toolkit (ORT) to analyze software projects and evaluate their open-source license compliance. The main ORT pipeline consist of four steps: Analyzer Scanner Evaluator Reporter After installing and configuring ORT, follow the above steps in order.","title":"Overview"},{"location":"ort/report/","text":"Report Create reports with the command: ort --info --force-overwrite report -i evaluator/evaluation-result.yml -o reporter \\ -f WebApp,NoticeTemplate \\ -O NoticeTemplate = template.path = ~/.ort/config/summary.ftl or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ report \\ -i /project/evaluator/evaluation-result.yml \\ -o /project/reporter \\ -f WebApp,NoticeTemplate \\ -O NoticeTemplate = template.path = /root/.ort/config/summary.ftl The Reporter outputs the reports in reporter/ .","title":"Report"},{"location":"ort/report/#report","text":"Create reports with the command: ort --info --force-overwrite report -i evaluator/evaluation-result.yml -o reporter \\ -f WebApp,NoticeTemplate \\ -O NoticeTemplate = template.path = ~/.ort/config/summary.ftl or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ report \\ -i /project/evaluator/evaluation-result.yml \\ -o /project/reporter \\ -f WebApp,NoticeTemplate \\ -O NoticeTemplate = template.path = /root/.ort/config/summary.ftl The Reporter outputs the reports in reporter/ .","title":"Report"},{"location":"ort/scan/","text":"Scan ORT Scanner is used to scan the source code of the dependencies to detect license and copyright findings in the source code. We recommend using ScanCode Toolkit as the scanner. Run scan with the command: ort --info --force-overwrite scan -i analyzer/analyzer-result.yml -o scanner or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ scan \\ -i /project/analyzer/analyzer-result.yml \\ -o /project/scanner The Scanner outputs the result in scanner/scan-result.yml , which can then be used as an input for the Evaluator.","title":"Scan"},{"location":"ort/scan/#scan","text":"ORT Scanner is used to scan the source code of the dependencies to detect license and copyright findings in the source code. We recommend using ScanCode Toolkit as the scanner. Run scan with the command: ort --info --force-overwrite scan -i analyzer/analyzer-result.yml -o scanner or with Docker: docker run -v $PWD :/project -v ~/.ort:/root/.ort --rm --env-file ~/.ort/config/.env \\ doubleopen/ort:latest --info --force-overwrite \\ scan \\ -i /project/analyzer/analyzer-result.yml \\ -o /project/scanner The Scanner outputs the result in scanner/scan-result.yml , which can then be used as an input for the Evaluator.","title":"Scan"},{"location":"yocto/","text":"Introduction Double Open aims to automate open source compliance for the Yocto Project . Overview Below is a high-level overview of the compliance workflow: sequenceDiagram autonumber Build Environment ->> Build Environment: Build the product with Yocto, create SPDX Document for the product Build Environment ->> Fossology: Upload source archives of the open-source components Fossology ->> Fossology: Scan the source for license and copyright data Build Environment ->> Fossology: Query for license and copyright data with file hash values Fossology ->> Build Environment: Return license and copyright data for the files Build Environment ->> Build Environment: Populate SPDX Document with the license and copyright data Build Environment ->> Build Environment: Convert SPDX Document to ORT's data format Build Environment ->> Build Environment: Evaluate compliance with ORT's Evaluator Build Environment ->> Build Environment: Create notice file and report with ORT's Reporter","title":"Introduction"},{"location":"yocto/#introduction","text":"Double Open aims to automate open source compliance for the Yocto Project .","title":"Introduction"},{"location":"yocto/#overview","text":"Below is a high-level overview of the compliance workflow: sequenceDiagram autonumber Build Environment ->> Build Environment: Build the product with Yocto, create SPDX Document for the product Build Environment ->> Fossology: Upload source archives of the open-source components Fossology ->> Fossology: Scan the source for license and copyright data Build Environment ->> Fossology: Query for license and copyright data with file hash values Fossology ->> Build Environment: Return license and copyright data for the files Build Environment ->> Build Environment: Populate SPDX Document with the license and copyright data Build Environment ->> Build Environment: Convert SPDX Document to ORT's data format Build Environment ->> Build Environment: Evaluate compliance with ORT's Evaluator Build Environment ->> Build Environment: Create notice file and report with ORT's Reporter","title":"Overview"},{"location":"yocto/evaluate/","text":"Evaluate OSS Review Toolkit is used to Evaluate the image's license compliance against a policy. The policy is defined with a license classification grouping licenses in categories and a rules script file. The license classifications file and rules script file should be stored in a configuration directory for ORT and be named license-classifications.yml and rules.kts respectively. This enables ORT to use them automatically without having to specify them separately. ORT_CONFIG_DIR = <POLICY_DIR> \\ ort evaluate \\ -i <ORT_RESULT> \\ -o <EVALUATOR_RESULT_DIR> \\ License classifications We maintain a license classifications file at Double Open's Policy Configuration repository . Rules An example of the rules file can be found in the ORT repository.","title":"Evaluate"},{"location":"yocto/evaluate/#evaluate","text":"OSS Review Toolkit is used to Evaluate the image's license compliance against a policy. The policy is defined with a license classification grouping licenses in categories and a rules script file. The license classifications file and rules script file should be stored in a configuration directory for ORT and be named license-classifications.yml and rules.kts respectively. This enables ORT to use them automatically without having to specify them separately. ORT_CONFIG_DIR = <POLICY_DIR> \\ ort evaluate \\ -i <ORT_RESULT> \\ -o <EVALUATOR_RESULT_DIR> \\","title":"Evaluate"},{"location":"yocto/evaluate/#license-classifications","text":"We maintain a license classifications file at Double Open's Policy Configuration repository .","title":"License classifications"},{"location":"yocto/evaluate/#rules","text":"An example of the rules file can be found in the ORT repository.","title":"Rules"},{"location":"yocto/fossology-migration/","text":"Migrate Fossology database The Fossology database can be migrated from an instance to another. This can be useful if the pipeline needs to be executed without access to the public Fossology instance. Below is a high-level diagram for what the pipeline looks like utilizing both the internal Fossology and Double Open's database. Provided that the tools are installed, Build Environment , Internal Fossology and Internal Storage do not need access to the public internet during the the build. After the build the source archives that contain source files not on Internal Fossology need to be uploaded to the Double Open Fossology over the internet, and the Internal Fossology needs to be updated from Double Open. sequenceDiagram autonumber rect rgb(191, 223, 255) note right of Build Environment: Compliance Pipeline Build Environment->>Build Environment: Build the project with Yocto Build Environment->>Internal Fossology: Query for license data Internal Fossology->>Build Environment: Return license data for existing files Build Environment->>Internal Storage: Upload source archives end Internal Storage->>Double Open Fossology: Upload missing source archives Double Open Fossology->>Double Open Fossology: New source files are concluded Double Open Fossology->>Internal Fossology: Update database Build Environment->>Build Environment: Rebuild with updated Fossology database Instructions Create archive from the existing database volume Get the database volume from existing Fossology instance and create an archive from the volume. The volume is expected to be named fossology_database . docker run --rm \\ --volume fossology_database:/dbdata:ro \\ --volume $PWD:/backup \\ ubuntu tar czvf /backup/$(date --rfc-3339=date)-database.tar.gz /dbdata Upload archive Upload to Digital Ocean Spaces: s3cmd put $(date --rfc-3339=date)-database.tar.gz s3://doubleopen Create a copy of the archive named newest-database.tar.gz : s3cmd cp s3://doubleopen/$(date --rfc-3339=date)-database.tar.gz s3://doubleopen/newest-database.tar.gz Download archive s3cmd get s3://doubleopen/newest-database.tar.gz Create a named Docker volume from the archived database Create a temporary container with a named volume. docker create \\ --volume fossy_migration:/dbdata \\ --name fossy_migration \\ busybox true Extract the database archive to the created volume. docker run --rm \\ --volumes-from fossy_migration \\ --volume $PWD:/backup:ro \\ ubuntu bash \\ -c \"cd /dbdata && tar xvf /backup/newest-database.tar.gz --strip 1\" Launch Fossology Clone docker-compose.yml for Fossology from https://github.com/doubleopen-project/fossology-migration . Create a .env file based on .env.example and fill the correct database credentials from the primary Fossolog. Launch Fossology with docker-compose up -d .","title":"Migrate Fossology database"},{"location":"yocto/fossology-migration/#migrate-fossology-database","text":"The Fossology database can be migrated from an instance to another. This can be useful if the pipeline needs to be executed without access to the public Fossology instance. Below is a high-level diagram for what the pipeline looks like utilizing both the internal Fossology and Double Open's database. Provided that the tools are installed, Build Environment , Internal Fossology and Internal Storage do not need access to the public internet during the the build. After the build the source archives that contain source files not on Internal Fossology need to be uploaded to the Double Open Fossology over the internet, and the Internal Fossology needs to be updated from Double Open. sequenceDiagram autonumber rect rgb(191, 223, 255) note right of Build Environment: Compliance Pipeline Build Environment->>Build Environment: Build the project with Yocto Build Environment->>Internal Fossology: Query for license data Internal Fossology->>Build Environment: Return license data for existing files Build Environment->>Internal Storage: Upload source archives end Internal Storage->>Double Open Fossology: Upload missing source archives Double Open Fossology->>Double Open Fossology: New source files are concluded Double Open Fossology->>Internal Fossology: Update database Build Environment->>Build Environment: Rebuild with updated Fossology database","title":"Migrate Fossology database"},{"location":"yocto/fossology-migration/#instructions","text":"","title":"Instructions"},{"location":"yocto/fossology-migration/#create-archive-from-the-existing-database-volume","text":"Get the database volume from existing Fossology instance and create an archive from the volume. The volume is expected to be named fossology_database . docker run --rm \\ --volume fossology_database:/dbdata:ro \\ --volume $PWD:/backup \\ ubuntu tar czvf /backup/$(date --rfc-3339=date)-database.tar.gz /dbdata","title":"Create archive from the existing database volume"},{"location":"yocto/fossology-migration/#upload-archive","text":"Upload to Digital Ocean Spaces: s3cmd put $(date --rfc-3339=date)-database.tar.gz s3://doubleopen Create a copy of the archive named newest-database.tar.gz : s3cmd cp s3://doubleopen/$(date --rfc-3339=date)-database.tar.gz s3://doubleopen/newest-database.tar.gz","title":"Upload archive"},{"location":"yocto/fossology-migration/#download-archive","text":"s3cmd get s3://doubleopen/newest-database.tar.gz","title":"Download archive"},{"location":"yocto/fossology-migration/#create-a-named-docker-volume-from-the-archived-database","text":"Create a temporary container with a named volume. docker create \\ --volume fossy_migration:/dbdata \\ --name fossy_migration \\ busybox true Extract the database archive to the created volume. docker run --rm \\ --volumes-from fossy_migration \\ --volume $PWD:/backup:ro \\ ubuntu bash \\ -c \"cd /dbdata && tar xvf /backup/newest-database.tar.gz --strip 1\"","title":"Create a named Docker volume from the archived database"},{"location":"yocto/fossology-migration/#launch-fossology","text":"Clone docker-compose.yml for Fossology from https://github.com/doubleopen-project/fossology-migration . Create a .env file based on .env.example and fill the correct database credentials from the primary Fossolog. Launch Fossology with docker-compose up -d .","title":"Launch Fossology"},{"location":"yocto/fossology/","text":"Get data from Fossology Upload source For Fossology to be able to return licensing and copyright data for the source files, they need to be uploaded to Fossology. The source archives to upload are filtered to exclude proprietary packages from being scanned by not uploading source archive for the recipe if the license of the recipe includes CLOSED . Uploading is done with Double Open CLI with the following command: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ upload \\ -f <FOSSOLOGY_FOLDER_ID> \\ --spdx <IMAGE_SPDX> \\ <SPDX_DEPLOY_DIR>/*.tar.bz2 Populate SPDX After the files have been scanned in Fossology, the SPDX Document of the image can be populated with the data from the Fossology API. The API is queried with tha SHA256 hash values of the source files. This is done with Double Open CLI with the following command: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ query \\ -i <IMAGE_SPDX> \\ -o <OUTPUT_SPDX>","title":"Get data from Fossology"},{"location":"yocto/fossology/#get-data-from-fossology","text":"","title":"Get data from Fossology"},{"location":"yocto/fossology/#upload-source","text":"For Fossology to be able to return licensing and copyright data for the source files, they need to be uploaded to Fossology. The source archives to upload are filtered to exclude proprietary packages from being scanned by not uploading source archive for the recipe if the license of the recipe includes CLOSED . Uploading is done with Double Open CLI with the following command: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ upload \\ -f <FOSSOLOGY_FOLDER_ID> \\ --spdx <IMAGE_SPDX> \\ <SPDX_DEPLOY_DIR>/*.tar.bz2","title":"Upload source"},{"location":"yocto/fossology/#populate-spdx","text":"After the files have been scanned in Fossology, the SPDX Document of the image can be populated with the data from the Fossology API. The API is queried with tha SHA256 hash values of the source files. This is done with Double Open CLI with the following command: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ query \\ -i <IMAGE_SPDX> \\ -o <OUTPUT_SPDX>","title":"Populate SPDX"},{"location":"yocto/generate-notices/","text":"Generate notices Notice files for the images are generated with ORT with the following command: ORT_CONFIG_DIR = <POLICY_DIR> \\ ort report \\ -i <EVALUATOR_RESULT_DIR>/evaluation-result.yml \\ -o <REPORT_DIR> \\ -f <FORMATS>","title":"Generate notices"},{"location":"yocto/generate-notices/#generate-notices","text":"Notice files for the images are generated with ORT with the following command: ORT_CONFIG_DIR = <POLICY_DIR> \\ ort report \\ -i <EVALUATOR_RESULT_DIR>/evaluation-result.yml \\ -o <REPORT_DIR> \\ -f <FORMATS>","title":"Generate notices"},{"location":"yocto/installation/","text":"Installation Double Open CLI can be downloaded from GitHub releases . We recommend using OSS Review Toolkit with Docker. Image for our fork of ORT can be found from DockerHub .","title":"Installation"},{"location":"yocto/installation/#installation","text":"Double Open CLI can be downloaded from GitHub releases . We recommend using OSS Review Toolkit with Docker. Image for our fork of ORT can be found from DockerHub .","title":"Installation"},{"location":"yocto/ort-conversion/","text":"Conversion The conversion from SPDX to ORT's data format is done with command in our fork of ORT . The command is part of the ORT's Helper CLI: orth convert-spdx-to-ort \\ -i <INPUT_SPDX> \\ -o <OUTPUT_ORT_FILE> \\ --repository-configuration-file <ORT.YML> The command creates a file mimicking ORT's scanner result which can be used to Evaluate the image's license compliance with ORT's Evaluator and to create reports with ORT's Reporter. To enable different rules for scanner findings and concluded licenses from Fossology, scanner findings are identified with LicenseRef-Scanner- prefix. This is not needed for notice generation, so the conversion for notice generation should be done with the following command: orth convert-spdx-to-ort \\ -i <INPUT_SPDX> \\ -o <OUTPUT_ORT_FILE> \\ --repository-configuration-file <ORT.YML> \\ --skip-scan If your policy does not require different rules for scanner results and concluded licenses, the evaluation can be performed with the --skip-scan conversion.","title":"Conversion"},{"location":"yocto/ort-conversion/#conversion","text":"The conversion from SPDX to ORT's data format is done with command in our fork of ORT . The command is part of the ORT's Helper CLI: orth convert-spdx-to-ort \\ -i <INPUT_SPDX> \\ -o <OUTPUT_ORT_FILE> \\ --repository-configuration-file <ORT.YML> The command creates a file mimicking ORT's scanner result which can be used to Evaluate the image's license compliance with ORT's Evaluator and to create reports with ORT's Reporter. To enable different rules for scanner findings and concluded licenses from Fossology, scanner findings are identified with LicenseRef-Scanner- prefix. This is not needed for notice generation, so the conversion for notice generation should be done with the following command: orth convert-spdx-to-ort \\ -i <INPUT_SPDX> \\ -o <OUTPUT_ORT_FILE> \\ --repository-configuration-file <ORT.YML> \\ --skip-scan If your policy does not require different rules for scanner results and concluded licenses, the evaluation can be performed with the --skip-scan conversion.","title":"Conversion"},{"location":"yocto/oss-review-toolkit/","text":"OSS Review Toolkit The workflow uses the OSS Review Toolkit for its Evaluator and Reporter modules. As meta-doubleopen creates SPDX Documents and ORT's Evaluator accepts ORT's own data model as an input, the SPDX needs to be converted to ORT's data model.","title":"OSS Review Toolkit"},{"location":"yocto/oss-review-toolkit/#oss-review-toolkit","text":"The workflow uses the OSS Review Toolkit for its Evaluator and Reporter modules. As meta-doubleopen creates SPDX Documents and ORT's Evaluator accepts ORT's own data model as an input, the SPDX needs to be converted to ORT's data model.","title":"OSS Review Toolkit"},{"location":"yocto/setup-yocto/","text":"Setup Yocto Double Open's layer, meta-doubleopen is used to create an SPDX Document describing the image built with Yocto. The layer needs to be added to conf/bblayers.conf and enabled with INHERIT += \"doubleopen\" in conf/local.conf . After meta-doubleopen is added, bitbake build <RECIPE> produces SPDX Documents for all packages in <SPDX_DEPLOY_DIR> (defaults to /build/tmp/deploy/spdx/ ) and the SPDX Document for the whole image is saved to DEPLOY_DIR_IMAGE .","title":"Setup Yocto"},{"location":"yocto/setup-yocto/#setup-yocto","text":"Double Open's layer, meta-doubleopen is used to create an SPDX Document describing the image built with Yocto. The layer needs to be added to conf/bblayers.conf and enabled with INHERIT += \"doubleopen\" in conf/local.conf . After meta-doubleopen is added, bitbake build <RECIPE> produces SPDX Documents for all packages in <SPDX_DEPLOY_DIR> (defaults to /build/tmp/deploy/spdx/ ) and the SPDX Document for the whole image is saved to DEPLOY_DIR_IMAGE .","title":"Setup Yocto"},{"location":"yocto/tldr/","text":"TLDR Add meta-doubleopen layer to Yocto's conf/bblayers.conf . Add INHERIT += \"doubleopen\" to Yocto's conf/local.conf . Build the image with Yocto. The resulting SPDX will be output to DEPLOY_DIR_IMAGE as <IMAGE_NAME>.spdx.json . Use CLI to upload missing packages to Fossology: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ upload \\ -s <SPDX_DEPLOY_DIR>/*.tar.bz2 \\ -f <FOSSOLOGY_FOLDER_ID> \\ --spdx <IMAGE_SPDX> Use CLI to populate the SPDX file with license data: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ query \\ -i <IMAGE_SPDX> \\ -o <OUTPUT_SPDX> Convert the SPDX file to ORT's format: orth convert-spdx-to-ort \\ -i <INPUT_SPDX> \\ -o <OUTPUT_ORT_FILE> \\ --repository-configuration-file <ORT.YML> Clone ORT configuration and policy. Evaluate: ORT_CONFIG_DIR = <POLICY_DIR> \\ ort evaluate \\ -i <ORT_RESULT> \\ -o <EVALUATOR_RESULT_DIR> \\ Generate reports and notices: ORT_CONFIG_DIR = <POLICY_DIR> \\ ort report \\ -i <EVALUATOR_RESULT_DIR>/evaluation-result.yml \\ -o <REPORT_DIR> \\ -f <FORMATS>","title":"TLDR"},{"location":"yocto/tldr/#tldr","text":"Add meta-doubleopen layer to Yocto's conf/bblayers.conf . Add INHERIT += \"doubleopen\" to Yocto's conf/local.conf . Build the image with Yocto. The resulting SPDX will be output to DEPLOY_DIR_IMAGE as <IMAGE_NAME>.spdx.json . Use CLI to upload missing packages to Fossology: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ upload \\ -s <SPDX_DEPLOY_DIR>/*.tar.bz2 \\ -f <FOSSOLOGY_FOLDER_ID> \\ --spdx <IMAGE_SPDX> Use CLI to populate the SPDX file with license data: doubleopen fossology -u <FOSSOLOGY_API_URI> -t <FOSSOLOGY_TOKEN> \\ query \\ -i <IMAGE_SPDX> \\ -o <OUTPUT_SPDX> Convert the SPDX file to ORT's format: orth convert-spdx-to-ort \\ -i <INPUT_SPDX> \\ -o <OUTPUT_ORT_FILE> \\ --repository-configuration-file <ORT.YML> Clone ORT configuration and policy. Evaluate: ORT_CONFIG_DIR = <POLICY_DIR> \\ ort evaluate \\ -i <ORT_RESULT> \\ -o <EVALUATOR_RESULT_DIR> \\ Generate reports and notices: ORT_CONFIG_DIR = <POLICY_DIR> \\ ort report \\ -i <EVALUATOR_RESULT_DIR>/evaluation-result.yml \\ -o <REPORT_DIR> \\ -f <FORMATS>","title":"TLDR"}]}